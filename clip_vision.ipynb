{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets accelerate matplotlib -U\n",
    "!pip install torch torchvision pillow\n",
    "!pip install wandb onnx -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c64b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai-clip\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install ftfy regex tqdm\n",
    "!python -m pip install setuptools==69.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64886e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b04b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = processor(Image.open('/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/val/2018-11-22 11.02.10.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CLIP model\n",
    "clip.available_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.tokenize(\"My name is antony\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb81a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "##### CLIP Training Dataset \n",
    "###############################\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import VisionTextDualEncoderModel, VisionTextDualEncoderProcessor\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Mapping from class ID to class name\n",
    "class_names = {\n",
    "    0: 'DUMPSTER', 1: 'VEHICLE', 2: 'SKID_STEER', 3: 'EXCAVATOR', 4: 'VAN',\n",
    "    5: 'LUMBER_BUNDLE', 6: 'CONE', 7: 'TRUCK', 8: 'GARBAGE_CONTAINER',\n",
    "    9: 'LADDER', 10: 'POWER_GENERATOR', 11: 'TELESCOPIC_HANDLER',\n",
    "    12: 'CONCRETE_BUCKET', 13: 'BOOMLIFT', 14: 'PLYWOOD', 15: 'TOILET_CABIN',\n",
    "    16: 'FORMWORK_PROP_BUNDLE', 17: 'CONDUIT_ROLL', 18: 'FORMWORK_PANEL',\n",
    "    19: 'CONCRETE_COLUMN', 20: 'PLATE_COMPACTOR', 21: 'TROWEL_POWER',\n",
    "    22: 'SLAB_SLEEVES', 23: 'MINI_EXCAVATOR', 24: 'CONTAINER', 25: 'SCISSORLIFT',\n",
    "    26: 'PICKUP_TRUCK', 27: 'MOBILE_CRANE', 28: 'EQUIPMENT', 29: 'TIEBACK_RIG',\n",
    "    30: 'TOWER_CRANE', 31: 'CONCRETE_PUMP', 32: 'DRILLRIG', 33: 'LOADER',\n",
    "    34: 'OFFICE_TRAILER', 35: 'DOZER', 36: 'BUS', 37: 'ROLLER', 38: 'CONCRETE_RIDE',\n",
    "    39: 'BACKHOE_LOADER', 40: 'FORKLIFT', 41: 'GRADER', 42: 'HAND_ROLLER',\n",
    "    43: 'HOIST_CABIN'\n",
    "}\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, text_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.text_dir = text_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.jpeg')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        text_filename = os.path.splitext(image_filename)[0] + '.txt'\n",
    "        text_path = os.path.join(self.text_dir, text_filename)\n",
    "\n",
    "\n",
    "        # Load and parse annotations\n",
    "        with open(text_path, 'r') as file:\n",
    "            annotations = file.readlines()\n",
    "        \n",
    "        parsed_annotations = [list(map(float, line.strip().split())) for line in annotations]\n",
    "\n",
    "        # Find the annotation with the largest area\n",
    "        correct_caption = \"unknown\"\n",
    "        if len(parsed_annotations) > 0:\n",
    "            parsed_annotations.sort(key=lambda x: x[3] * x[4], reverse=True)\n",
    "            top_annotations = parsed_annotations[:2]\n",
    "            caption_indices = [int(ann[0]) for ann in top_annotations]\n",
    "            correct_captions = [class_names[idx] for idx in caption_indices]\n",
    "            correct_caption = ' '.join(correct_captions)\n",
    "\n",
    "        \n",
    "        image = preprocess(Image.open(image_path))\n",
    "        correct_caption_tensor = clip.tokenize(correct_caption).squeeze()\n",
    "        return image, correct_caption_tensor, image_path, correct_caption\n",
    "\n",
    "image_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train'\n",
    "text_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/labels/train'\n",
    "\n",
    "dataset = CustomDataset(image_dir=image_dir, text_dir=text_dir)\n",
    "\n",
    "# To use with a DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=50)\n",
    "\n",
    "# # Example of iterating through the dataset\n",
    "for batch in dataloader:\n",
    "    #print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73081d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(10,dtype=torch.long,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6372cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad29596",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "##### Contrastive Training  \n",
    "###############################\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        if p.requires_grad:\n",
    "          p.grad.data = p.grad.data.float() \n",
    "        \n",
    "        \n",
    "import torch\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a simple training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "model.to(device)\n",
    "clip.model.convert_weights(model)\n",
    "model.eval()\n",
    "\n",
    "learning_rate=1e-5\n",
    "weight_decay=0.001\n",
    "betas=(0.9,0.98)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,betas=betas,eps=1e-6,weight_decay=weight_decay) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "num_epochs = 5\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "########Log to Wandb\n",
    "config = dict(epochs=num_epochs,learning_rate=learning_rate, weight_decay= weight_decay, betas= betas)\n",
    "wandb.init(project=\"open-ai-clip-finetuning\", config=config)\n",
    "wandb.watch(model, optimizer, log=\"all\", log_freq=10)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    tqdm_object = tqdm(dataloader, total=len(dataloader))\n",
    "    nb_batches = len(dataloader)\n",
    "    for i, batch in enumerate(tqdm_object):\n",
    "        optimizer.zero_grad()\n",
    "        images, texts, _, _ = batch\n",
    "        \n",
    "        # Move images and texts to the specified device (CPU or GPU)\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        \n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "\n",
    "        tqdm_object.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": total_loss})\n",
    "        \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "save_path = \"./clip-finetune\"\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"{save_path}\") #just change to your preferred folder/filename\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./clip-finetune\"\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"{save_path}\") #just change to your preferred folder/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "save_path = \"./clip-finetune\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device) #Must set jit=False for training\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Single Image\n",
    "#################################\n",
    "import requests\n",
    "from PIL import Image \n",
    "\n",
    "image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_001--Optima--09-02-2018-0631.jpg\"\n",
    "image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_001--Optima--14-11-2017-5846.jpg\"\n",
    "#image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_002AFA69-A930-41C6-8982-20000E50EF97.jpeg\"\n",
    "#image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_003--Optima--12-08-2017.jpg\"\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "smaller_image = image.resize((int(image.width / 2), int(image.height / 2)))\n",
    "\n",
    "# Display the smaller image\n",
    "display(smaller_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#### Inference with Contrastive \n",
    "###############################\n",
    "from transformers import VisionTextDualEncoderModel, BertTokenizer\n",
    "from PIL import Image\n",
    "import torch \n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the saved model and processor\n",
    "# model = CLIPModel.from_pretrained(save_path)\n",
    "# processor = CLIPProcessor.from_pretrained(save_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define a function for inference\n",
    "def predict(image_path, captions):\n",
    "    # Preprocess the image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize(captions).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(image, text)\n",
    "\n",
    "    return logits_per_image, logits_per_text\n",
    "\n",
    "# Example usage\n",
    "#image_path = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/val/20210325175112_production_2746262081.jpeg'\n",
    "#image_path = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_002AFA69-A930-41C6-8982-20000E50EF97.jpeg'\n",
    "captions = list(class_names.values())\n",
    "\n",
    "logits_per_image, logits_per_text = predict(image_path, captions)\n",
    "correct_probs = F.softmax(logits_per_image, dim=-1)\n",
    "print(logits_per_image)\n",
    "#correct_label = torch.argmax(correct_probs, dim=-1).item()\n",
    "correct_label_indices =torch.topk(correct_probs, 5).indices.squeeze().tolist()\n",
    "correct_classes = [class_names[i] for i in correct_label_indices]\n",
    "\n",
    "print(correct_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97972661",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(correct_probs, 5).indices.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ceb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Calculate Accuracy, MRR and Top K \n",
    "#################################\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import time \n",
    "# Assuming CustomImageTextDataset, generate_caption, model, and tokenizer are defined elsewhere\n",
    "\n",
    "image_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/val'\n",
    "text_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/labels/val'\n",
    "\n",
    "val_dataset = CustomDataset(image_dir=image_dir, text_dir=text_dir)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(val_dataset, model, k=3, total=200):\n",
    "    matched = 0\n",
    "    total_samples = 0\n",
    "    mrr_total = 0.0\n",
    "    top_k_matched = 0\n",
    "    total_inference_time = 0.0\n",
    "\n",
    "    for i in range(total):\n",
    "        original_labels = val_dataset[i][3]\n",
    "        image_path = val_dataset[i][2]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        captions = list(class_names.values())\n",
    "\n",
    "        start_time = time.time()\n",
    "        logits_per_image, logits_per_text = predict(image_path, captions)\n",
    "        inference_time = time.time() - start_time\n",
    "        total_inference_time += inference_time\n",
    "\n",
    "        correct_probs = F.softmax(logits_per_image, dim=-1)\n",
    "        #correct_label = torch.argmax(correct_probs, dim=-1).item()\n",
    "        correct_label_indices =torch.topk(correct_probs, 3).indices.squeeze().tolist()\n",
    "        predicted_labels = [class_names[i] for i in correct_label_indices]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        for clazz in predicted_labels:\n",
    "            if clazz in original_labels: \n",
    "                matched += 1\n",
    "                break\n",
    "\n",
    "        # Calculate MRR\n",
    "        reciprocal_rank = 0.0\n",
    "        for rank, predicted_label in enumerate(predicted_labels, start=1):\n",
    "            if predicted_label in original_labels:\n",
    "                reciprocal_rank = 1.0 / rank\n",
    "                break\n",
    "        mrr_total += reciprocal_rank\n",
    "\n",
    "        # Calculate Top-K\n",
    "        if any(predicted_label in original_labels for predicted_label in predicted_labels[:k]):\n",
    "            top_k_matched += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        print(f\"original: {original_labels} vs predicted {predicted_labels}\")\n",
    "\n",
    "    accuracy = matched / total_samples\n",
    "    mrr = mrr_total / total_samples\n",
    "    top_k_accuracy = top_k_matched / total_samples\n",
    "    average_inference_time = total_inference_time / total_samples\n",
    "    return accuracy, mrr, top_k_accuracy, average_inference_time\n",
    "\n",
    "# Example usage\n",
    "accuracy, mrr, top_k_accuracy, average_inference_time = calculate_metrics(val_dataset, model, k=5, total=200)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr}\")\n",
    "print(f\"Top-5 Accuracy: {top_k_accuracy}\")\n",
    "print(f\"Average Inference Time: {average_inference_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae230fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_outputs(image_path, captions):\n",
    "    # Preprocess the image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize(captions).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        print(image_features)\n",
    "        print(text_features)\n",
    "        print(image_features * text_features)\n",
    "        return model(image, text)\n",
    "    \n",
    "captions = list(class_names.values())\n",
    "outputs = model_outputs(image_path, captions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76727621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
