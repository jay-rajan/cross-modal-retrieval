{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets accelerate matplotlib -U\n",
    "!pip install torch torchvision pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c64b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64886e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionTextDualEncoderModel, VisionTextDualEncoderProcessor, AutoTokenizer, AutoImageProcessor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the pre-trained models\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb81a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "##### Contrastive Dataset \n",
    "###############################\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import VisionTextDualEncoderModel, VisionTextDualEncoderProcessor\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Mapping from class ID to class name\n",
    "class_names = {\n",
    "    0: 'DUMPSTER', 1: 'VEHICLE', 2: 'SKID_STEER', 3: 'EXCAVATOR', 4: 'VAN',\n",
    "    5: 'LUMBER_BUNDLE', 6: 'CONE', 7: 'TRUCK', 8: 'GARBAGE_CONTAINER',\n",
    "    9: 'LADDER', 10: 'POWER_GENERATOR', 11: 'TELESCOPIC_HANDLER',\n",
    "    12: 'CONCRETE_BUCKET', 13: 'BOOMLIFT', 14: 'PLYWOOD', 15: 'TOILET_CABIN',\n",
    "    16: 'FORMWORK_PROP_BUNDLE', 17: 'CONDUIT_ROLL', 18: 'FORMWORK_PANEL',\n",
    "    19: 'CONCRETE_COLUMN', 20: 'PLATE_COMPACTOR', 21: 'TROWEL_POWER',\n",
    "    22: 'SLAB_SLEEVES', 23: 'MINI_EXCAVATOR', 24: 'CONTAINER', 25: 'SCISSORLIFT',\n",
    "    26: 'PICKUP_TRUCK', 27: 'MOBILE_CRANE', 28: 'EQUIPMENT', 29: 'TIEBACK_RIG',\n",
    "    30: 'TOWER_CRANE', 31: 'CONCRETE_PUMP', 32: 'DRILLRIG', 33: 'LOADER',\n",
    "    34: 'OFFICE_TRAILER', 35: 'DOZER', 36: 'BUS', 37: 'ROLLER', 38: 'CONCRETE_RIDE',\n",
    "    39: 'BACKHOE_LOADER', 40: 'FORKLIFT', 41: 'GRADER', 42: 'HAND_ROLLER',\n",
    "    43: 'HOIST_CABIN'\n",
    "}\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, text_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.text_dir = text_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.jpeg')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        text_filename = os.path.splitext(image_filename)[0] + '.txt'\n",
    "        text_path = os.path.join(self.text_dir, text_filename)\n",
    "\n",
    "\n",
    "        # Load and parse annotations\n",
    "        with open(text_path, 'r') as file:\n",
    "            annotations = file.readlines()\n",
    "        \n",
    "        parsed_annotations = [list(map(float, line.strip().split())) for line in annotations]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Find the annotation with the largest area\n",
    "        correct_caption = \"unknown\"\n",
    "        if len(parsed_annotations) > 0:\n",
    "            parsed_annotations.sort(key=lambda x: x[3] * x[4], reverse=True)\n",
    "            top_annotations = parsed_annotations[:2]\n",
    "            caption_indices = [int(ann[0]) for ann in top_annotations]\n",
    "            correct_captions = [class_names[idx] for idx in caption_indices]\n",
    "            correct_caption = ' '.join(correct_captions)\n",
    "        \n",
    "        inputs = processor(text=[correct_caption], images=[image], return_tensors=\"pt\", padding=\"max_length\")\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "image_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train'\n",
    "text_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/labels/train'\n",
    "\n",
    "dataset = CustomDataset(image_dir=image_dir, text_dir=text_dir)\n",
    "\n",
    "# To use with a DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=50)\n",
    "\n",
    "# # Example of iterating through the dataset\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[343][\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292da292",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple training loop\n",
    "from transformers import VisionTextDualEncoderModel, VisionTextDualEncoderProcessor, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the pre-trained models\n",
    "vision_model_name = \"openai/clip-vit-base-patch32\"\n",
    "text_model_name = \"bert-/base-uncased\"\n",
    "\n",
    "model = model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
    "    \"google/vit-base-patch16-224\", \"google-bert/bert-base-uncased\"\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad29596",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "##### Contrastive Training  \n",
    "###############################\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a simple training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-7,betas=(0.9,0.98),eps=1e-6,weight_decay=0.001)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    tqdm_object = tqdm(dataloader, total=len(dataloader),position=0, leave=True)\n",
    "    nb_batches = len(dataloader)\n",
    "    for i, batch in enumerate(tqdm_object):\n",
    "        # Move inputs to the GPU if available\n",
    "        correct_inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "            \"pixel_values\": batch[\"pixel_values\"].to(device),\n",
    "            \"return_loss\":True\n",
    "        }\n",
    "        # Forward pass for correct pairs\n",
    "        correct_outputs = model(**correct_inputs)\n",
    "        logits_per_image_correct = correct_outputs.logits_per_image\n",
    "        logits_per_text_correct = correct_outputs.logits_per_text\n",
    "\n",
    "        # Contrastive loss\n",
    "        labels = torch.arange(logits_per_image_correct.size(0)).to(device)\n",
    "        loss_image_correct = F.cross_entropy(logits_per_image_correct, labels)\n",
    "        loss_text_correct = F.cross_entropy(logits_per_text_correct, labels)\n",
    "\n",
    "        loss = (loss_image_correct + loss_text_correct) / 2 \n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        tqdm_object.set_postfix(\n",
    "            batch=\"{}/{}\".format(i+1,nb_batches),\n",
    "            train_loss=loss.item(),\n",
    "            lr=5e-5\n",
    "        )\n",
    "        \n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "model.save_pretrained(\"./vit-bert\")\n",
    "processor.save_pretrained(\"./vit-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./vit-bert\")\n",
    "processor.save_pretrained(\"./vit-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/val/2018-11-22 11.02.10.jpg'\n",
    "#image_path = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/val/01_4A799364-FADA-4722-BC9C-59D4C913B168.jpeg'\n",
    "#image_path = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_002AFA69-A930-41C6-8982-20000E50EF97.jpeg'\n",
    "#image_path = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/val/20210325175112_production_2746262081.jpeg'\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#### Inference with Contrastive \n",
    "###############################\n",
    "from transformers import VisionTextDualEncoderModel, BertTokenizer\n",
    "from PIL import Image\n",
    "import torch \n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the saved model and processor\n",
    "model = VisionTextDualEncoderModel.from_pretrained(\"./vit-bert\")\n",
    "processor = VisionTextDualEncoderProcessor.from_pretrained(\"./vit-bert\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define a function for inference\n",
    "def predict(image_path, captions):\n",
    "    # Preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=captions, images=[image], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    pixes_values = inputs.pixel_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixes_values, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "\n",
    "    return logits_per_image, logits_per_text\n",
    "\n",
    "captions = list(class_names.values())\n",
    "\n",
    "logits_per_image, logits_per_text = predict(image_path, captions)\n",
    "correct_probs = F.softmax(logits_per_image, dim=-1)\n",
    "print(logits_per_image)\n",
    "#correct_label = torch.argmax(correct_probs, dim=-1).item()\n",
    "correct_label_indices =torch.topk(correct_probs, 5).indices.squeeze().tolist()\n",
    "correct_classes = [class_names[i] for i in correct_label_indices]\n",
    "\n",
    "print(correct_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97972661",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(correct_probs, 5).indices.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Calculate Accuracy\n",
    "#################################\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def extract_top(scores, n):\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_n = sorted_scores[:n]\n",
    "    captions = [cap[0] for cap in top_n]\n",
    "    return captions\n",
    "\n",
    "matched = 0\n",
    "total = 200\n",
    "for i in range(total):\n",
    "    original_captions = dataset[i][0]\n",
    "    image_path = dataset[i][3]\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    captions = list(class_names.values())\n",
    "\n",
    "    logits_per_image, logits_per_text = predict(image_path, captions)\n",
    "    correct_probs = F.softmax(logits_per_image, dim=-1)\n",
    "    #correct_label = torch.argmax(correct_probs, dim=-1).item()\n",
    "    correct_label_indices =torch.topk(correct_probs, 5).indices.squeeze().tolist()\n",
    "    correct_classes = [class_names[i] for i in correct_label_indices]\n",
    "    \n",
    "    \n",
    "    for clazz in correct_classes:\n",
    "        if clazz in original_captions: \n",
    "            matched += 1\n",
    "    \n",
    "    print(f\"original: {original_captions} vs predicted {correct_classes}\")\n",
    "    \n",
    "accuracy = matched/total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ceb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569a2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
