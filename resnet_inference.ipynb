{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch torchvision\n",
    "!pip install -U matplotlib\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d47d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f6088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "###Initialize Dataset \n",
    "#################################\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import VisionTextDualEncoderModel, VisionTextDualEncoderProcessor\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "class_names = {\n",
    "    0: 'DUMPSTER', 1: 'VEHICLE', 2: 'SKID_STEER', 3: 'EXCAVATOR', 4: 'VAN',\n",
    "    5: 'LUMBER_BUNDLE', 6: 'CONE', 7: 'TRUCK', 8: 'GARBAGE_CONTAINER',\n",
    "    9: 'LADDER', 10: 'POWER_GENERATOR', 11: 'TELESCOPIC_HANDLER',\n",
    "    12: 'CONCRETE_BUCKET', 13: 'BOOMLIFT', 14: 'PLYWOOD', 15: 'TOILET_CABIN',\n",
    "    16: 'FORMWORK_PROP_BUNDLE', 17: 'CONDUIT_ROLL', 18: 'FORMWORK_PANEL',\n",
    "    19: 'CONCRETE_COLUMN', 20: 'PLATE_COMPACTOR', 21: 'TROWEL_POWER',\n",
    "    22: 'SLAB_SLEEVES', 23: 'MINI_EXCAVATOR', 24: 'CONTAINER', 25: 'SCISSORLIFT',\n",
    "    26: 'PICKUP_TRUCK', 27: 'MOBILE_CRANE', 28: 'EQUIPMENT', 29: 'TIEBACK_RIG',\n",
    "    30: 'TOWER_CRANE', 31: 'CONCRETE_PUMP', 32: 'DRILLRIG', 33: 'LOADER',\n",
    "    34: 'OFFICE_TRAILER', 35: 'DOZER', 36: 'BUS', 37: 'ROLLER', 38: 'CONCRETE_RIDE',\n",
    "    39: 'BACKHOE_LOADER', 40: 'FORKLIFT', 41: 'GRADER', 42: 'HAND_ROLLER',\n",
    "    43: 'HOIST_CABIN', 44: \"UNKNOWN\"\n",
    "}\n",
    "\n",
    "def convert_to_words(text):\n",
    "    words = text.split('_')\n",
    "    lowercase_words = [word.lower() for word in words]\n",
    "    return ' '.join(lowercase_words)\n",
    "\n",
    "image_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train'\n",
    "text_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/labels/train'\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, text_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.text_dir = text_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.jpeg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        text_filename = os.path.splitext(image_filename)[0] + '.txt'\n",
    "        text_path = os.path.join(self.text_dir, text_filename)\n",
    "\n",
    "        # Load and parse annotations\n",
    "        with open(text_path, 'r') as file:\n",
    "            annotations = file.readlines()\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        parsed_annotations = [list(map(float, line.strip().split())) for line in annotations]\n",
    "\n",
    "        caption_indices = [44]\n",
    "        if len(parsed_annotations) > 0:\n",
    "            parsed_annotations.sort(key=lambda x: x[3] * x[4], reverse=True)\n",
    "            top_annotations = parsed_annotations[:5]\n",
    "            caption_indices = [int(ann[0]) for ann in top_annotations]\n",
    "        \n",
    "        captions = [convert_to_words(class_names[idx]) for idx in caption_indices]    \n",
    "        caption = ' '.join(captions)\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"captions\": caption,\n",
    "            \"image_path\": image_path\n",
    "        }\n",
    "\n",
    "dataset = CustomDataset(image_dir=image_dir, text_dir=text_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029fd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacfae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Calculat Accuracy, MRR, TopK \n",
    "#################################\n",
    "\n",
    "import torch\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time \n",
    "# Load the pre-trained model and image processor\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "image_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/val'\n",
    "text_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/labels/val'\n",
    "\n",
    "dataset = CustomDataset(image_dir=image_dir, text_dir=text_dir)\n",
    "\n",
    "def calculate_metrics(dataset, model, processor, device, k=3, total=200):\n",
    "    matched = 0\n",
    "    total_samples = 0\n",
    "    mrr_total = 0.0\n",
    "    top_k_matched = 0\n",
    "    total_inference_time = 0.0\n",
    "\n",
    "    for i in range(total):\n",
    "        original_captions = dataset[i][\"captions\"]\n",
    "        image_path = dataset[i][\"image_path\"]\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        inputs.to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        outputs = model(**inputs)\n",
    "        inference_time = time.time() - start_time\n",
    "        total_inference_time += inference_time\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Convert the bounding boxes to the original image scale\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        post_processed = processor.post_process_object_detection(outputs, target_sizes=target_sizes)[0]\n",
    "        bboxes = post_processed[\"boxes\"]\n",
    "        labels = post_processed[\"labels\"]\n",
    "        scores = post_processed[\"scores\"]\n",
    "\n",
    "        # Calculate areas of bounding boxes\n",
    "        areas = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])\n",
    "        \n",
    "        predicted_captions = []\n",
    "        if len(areas) > 0:\n",
    "            top_k_indices = torch.topk(scores, min(k,len(areas))).indices.tolist()\n",
    "            id2label = model.config.id2label\n",
    "            predicted_captions = [id2label[labels[idx].item()] for idx in top_k_indices]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        for match in predicted_captions:\n",
    "            if match in original_captions: \n",
    "                matched += 1\n",
    "                break\n",
    "\n",
    "        # Calculate MRR\n",
    "        reciprocal_rank = 0.0\n",
    "        for rank, predicted_label in enumerate(predicted_captions, start=1):\n",
    "            if predicted_label in original_captions:\n",
    "                reciprocal_rank = 1.0 / rank\n",
    "                break\n",
    "        mrr_total += reciprocal_rank\n",
    "\n",
    "        # Calculate Top-K accuracy\n",
    "        if any(predicted_label in original_captions for predicted_label in predicted_captions[:k]):\n",
    "            top_k_matched += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        print(f\"original: {original_captions} vs predicted {predicted_captions}\")\n",
    "\n",
    "    accuracy = matched / total_samples\n",
    "    mrr = mrr_total / total_samples\n",
    "    top_k_accuracy = top_k_matched / total_samples\n",
    "    average_inference_time = total_inference_time / total_samples\n",
    "    return accuracy, mrr, top_k_accuracy, average_inference_time\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "accuracy, mrr, top_k_accuracy, average_inference_time = calculate_metrics(dataset, model, processor, device, k=3, total=200)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr}\")\n",
    "print(f\"Top-3 Accuracy: {top_k_accuracy}\")\n",
    "print(f\"Average Inference Time: {average_inference_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Perform Single inference \n",
    "#################################\n",
    "\n",
    "import torch\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Load the pre-trained model and image processor\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Load an image\n",
    "image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_001--Optima--09-02-2018-0631.jpg\"\n",
    "#image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_001--Optima--14-11-2017-5846.jpg\"\n",
    "#image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_002AFA69-A930-41C6-8982-20000E50EF97.jpeg\"\n",
    "image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_003--Optima--12-08-2017.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Prepare the image for the model\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract outputs\n",
    "logits = outputs.logits\n",
    "bboxes = outputs.pred_boxes\n",
    "\n",
    "# Get the probabilities and the predicted class labels\n",
    "probs = logits.softmax(-1)[0, :, :-1]\n",
    "\n",
    "# Convert the bounding boxes to the original image scale\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "post_processed = processor.post_process_object_detection(outputs, target_sizes=target_sizes)[0]\n",
    "post_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the image with bounding boxes\n",
    "fig, ax = plt.subplots(1, figsize=(16, 10))\n",
    "ax.imshow(image)\n",
    "\n",
    "# Define the threshold for detection\n",
    "threshold = 0.9\n",
    "\n",
    "for score, label, box in zip(post_processed['scores'], post_processed['labels'], post_processed['boxes']):\n",
    "    if score >= threshold:\n",
    "        xmin, ymin, xmax, ymax = box.tolist()\n",
    "        width, height = xmax - xmin, ymax - ymin\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        \n",
    "        # Add the rectangle to the plot\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(xmin, ymin, f\"{model.config.id2label[label.item()]}: {score:.2f}\", fontsize=12, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdae58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
