{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bcea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb onnx -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462970a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9bbfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d515ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "##### inference with out of box model\n",
    "###############################\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# load a fine-tuned image captioning model and corresponding tokenizer and image processor\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# let's perform inference on an image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image_path = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/val/01_4A799364-FADA-4722-BC9C-59D4C913B168.jpeg'\n",
    "\n",
    "image = Image.open(image_path)\n",
    "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# autoregressively generate caption (uses greedy decoding by default)\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", \"google-bert/bert-base-uncased\"\n",
    ")\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "###Initialize Dataset \n",
    "#################################\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import VisionTextDualEncoderModel, VisionTextDualEncoderProcessor, AdamW, DataCollatorWithPadding\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Mapping from class ID to class name\n",
    "class_names = {\n",
    "    0: 'DUMPSTER', 1: 'VEHICLE', 2: 'SKID_STEER', 3: 'EXCAVATOR', 4: 'VAN',\n",
    "    5: 'LUMBER_BUNDLE', 6: 'CONE', 7: 'TRUCK', 8: 'GARBAGE_CONTAINER',\n",
    "    9: 'LADDER', 10: 'POWER_GENERATOR', 11: 'TELESCOPIC_HANDLER',\n",
    "    12: 'CONCRETE_BUCKET', 13: 'BOOMLIFT', 14: 'PLYWOOD', 15: 'TOILET_CABIN',\n",
    "    16: 'FORMWORK_PROP_BUNDLE', 17: 'CONDUIT_ROLL', 18: 'FORMWORK_PANEL',\n",
    "    19: 'CONCRETE_COLUMN', 20: 'PLATE_COMPACTOR', 21: 'TROWEL_POWER',\n",
    "    22: 'SLAB_SLEEVES', 23: 'MINI_EXCAVATOR', 24: 'CONTAINER', 25: 'SCISSORLIFT',\n",
    "    26: 'PICKUP_TRUCK', 27: 'MOBILE_CRANE', 28: 'EQUIPMENT', 29: 'TIEBACK_RIG',\n",
    "    30: 'TOWER_CRANE', 31: 'CONCRETE_PUMP', 32: 'DRILLRIG', 33: 'LOADER',\n",
    "    34: 'OFFICE_TRAILER', 35: 'DOZER', 36: 'BUS', 37: 'ROLLER', 38: 'CONCRETE_RIDE',\n",
    "    39: 'BACKHOE_LOADER', 40: 'FORKLIFT', 41: 'GRADER', 42: 'HAND_ROLLER',\n",
    "    43: 'HOIST_CABIN'\n",
    "}\n",
    "\n",
    "def to_captions(name):\n",
    "    return \" \".join(name.lower().split('_'))\n",
    "\n",
    "class CustomImageTextDataset(Dataset):\n",
    "    def __init__(self, image_dir, text_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.text_dir = text_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.jpeg')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        text_filename = os.path.splitext(image_filename)[0] + '.txt'\n",
    "        text_path = os.path.join(self.text_dir, text_filename)\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        \n",
    "        # Load and parse annotations\n",
    "        with open(text_path, 'r') as file:\n",
    "            annotations = file.readlines()\n",
    "        \n",
    "        parsed_annotations = [list(map(float, line.strip().split())) for line in annotations]\n",
    "        # Find the annotation with the largest area\n",
    "        correct_caption = \"an image of unknown\"\n",
    "        if len(parsed_annotations) > 0:\n",
    "            parsed_annotations.sort(key=lambda x: x[3] * x[4], reverse=True)\n",
    "            top_annotations = parsed_annotations[:4]\n",
    "            caption_indices = [int(ann[0]) for ann in top_annotations]\n",
    "            correct_captions = [class_names[idx] for idx in caption_indices]\n",
    "            correct_caption = ' '.join(correct_captions)\n",
    "            correct_caption = f\"an image of {to_captions(correct_caption)}\"\n",
    "\n",
    "        pixel_values = image_processor(images=image, return_tensors='pt').pixel_values.squeeze()\n",
    "        labels = tokenizer(correct_caption, return_tensors='pt').input_ids.squeeze()\n",
    "        return {'pixel_values': pixel_values, 'labels': labels, 'image_path': image_path, 'captions': correct_caption}\n",
    "\n",
    "image_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train'\n",
    "text_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/labels/train'\n",
    "\n",
    "train_dataset = CustomImageTextDataset(image_dir=image_dir, text_dir=text_dir)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels_padded\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# # Example of iterating through the dataset\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "batch_size=4\n",
    "learning_rate = 5e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "########Log to Wandb\n",
    "config = dict(epochs=num_epochs,learning_rate=learning_rate)\n",
    "wandb.init(project=\"vision-encoder-decoder-finetuning\", config=config)\n",
    "wandb.watch(model, optimizer, log=\"all\", log_freq=10)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} loss: {epoch_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('vision_encoder_decoder/model')\n",
    "image_processor.save_pretrained('vision_encoder_decoder/feature_extractor')\n",
    "tokenizer.save_pretrained('vision_encoder_decoder/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Single Image\n",
    "#################################\n",
    "import requests\n",
    "from PIL import Image \n",
    "\n",
    "image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_001--Optima--09-02-2018-0631.jpg\"\n",
    "image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_001--Optima--14-11-2017-5846.jpg\"\n",
    "#image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_002AFA69-A930-41C6-8982-20000E50EF97.jpeg\"\n",
    "#image_path = \"/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/train/01_003--Optima--12-08-2017.jpg\"\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "smaller_image = image.resize((int(image.width / 2), int(image.height / 2)))\n",
    "\n",
    "# Display the smaller image\n",
    "display(smaller_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2dcd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "###Inference with trained model \n",
    "########################################\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model, feature extractor, and tokenizer\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"vision_encoder_decoder/model\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"vision_encoder_decoder/feature_extractor\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vision_encoder_decoder/tokenizer\")\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.generation_config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    pixel_values = image_processor(images=image, return_tensors='pt').pixel_values\n",
    "    return pixel_values\n",
    "\n",
    "def generate_caption(pixel_values, model, tokenizer, max_length=64, num_beams=4):\n",
    "    # Generate text\n",
    "    output_ids = model.generate(pixel_values)\n",
    "    # Decode the output ids to text\n",
    "    caption = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    return caption\n",
    "\n",
    "pixel_values = preprocess_image(image_path)\n",
    "\n",
    "caption = generate_caption(pixel_values, model, tokenizer)\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Calculate Accuracy, MRR and Top K \n",
    "#################################\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "# Assuming CustomImageTextDataset, preprocess_image, generate_caption, model, and tokenizer are defined elsewhere\n",
    "\n",
    "image_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/images/val'\n",
    "text_dir = '/mnt/data/ypatel/ObjectDetection/Dataset/Dataset_D8/labels/val'\n",
    "\n",
    "val_dataset = CustomImageTextDataset(image_dir=image_dir, text_dir=text_dir)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(val_dataset, model, tokenizer, preprocess_image, k=3, total=200):\n",
    "    matched = 0\n",
    "    total_samples = 0\n",
    "    mrr_total = 0.0\n",
    "    top_k_matched = 0\n",
    "    total_inference_time = 0.0\n",
    "\n",
    "    for i in range(total):\n",
    "        original_labels = val_dataset[i][\"captions\"]\n",
    "        image_path = val_dataset[i]['image_path']\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        pixel_values = preprocess_image(image_path)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        caption = generate_caption(pixel_values, model, tokenizer)\n",
    "        inference_time = time.time() - start_time\n",
    "        total_inference_time += inference_time\n",
    "\n",
    "\n",
    "        predicted_labels = caption.split()\n",
    "        \n",
    "        # take only afte an image of \n",
    "        original_labels = original_labels.split()[3:]\n",
    "        predicted_labels = predicted_labels[3:]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        for clazz in predicted_labels:\n",
    "            if clazz in original_labels: \n",
    "                matched += 1\n",
    "                break\n",
    "\n",
    "        # Calculate MRR\n",
    "        reciprocal_rank = 0.0\n",
    "        for rank, predicted_label in enumerate(predicted_labels, start=1):\n",
    "            if predicted_label in original_labels:\n",
    "                reciprocal_rank = 1.0 / rank\n",
    "                break\n",
    "        mrr_total += reciprocal_rank\n",
    "\n",
    "        # Calculate Top-K\n",
    "        if any(predicted_label in original_labels for predicted_label in predicted_labels[:k]):\n",
    "            top_k_matched += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        print(f\"original: {original_labels} vs predicted {predicted_labels}\")\n",
    "\n",
    "    accuracy = matched / total_samples\n",
    "    mrr = mrr_total / total_samples\n",
    "    top_k_accuracy = top_k_matched / total_samples\n",
    "    average_inference_time = total_inference_time / total_samples\n",
    "\n",
    "    return accuracy, mrr, top_k_accuracy, average_inference_time\n",
    "\n",
    "# Example usage\n",
    "accuracy, mrr, top_k_accuracy, average_inference_time = calculate_metrics(val_dataset, model, tokenizer, preprocess_image, k=5, total=200)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr}\")\n",
    "print(f\"Top-5 Accuracy: {top_k_accuracy}\")\n",
    "print(f\"Average Inference Time: {average_inference_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8e441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
